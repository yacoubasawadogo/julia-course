# Session 11 : Statistiques et Machine Learning avec MLJ.jl

## üéØ Objectifs de la Session
- Comprendre les concepts fondamentaux des statistiques et du machine learning
- Ma√Ætriser l'√©cosyst√®me MLJ.jl pour l'analyse pr√©dictive
- Impl√©menter des mod√®les de r√©gression lin√©aire et multiple
- Effectuer de la classification avec diff√©rents algorithmes
- √âvaluer et valider les performances des mod√®les
- Appliquer le ML √† des probl√©matiques concr√®tes du Burkina Faso

## üìä Introduction aux Statistiques et Machine Learning

### Qu'est-ce que le Machine Learning ?
Le Machine Learning (apprentissage automatique) permet aux ordinateurs d'apprendre des patterns dans les donn√©es sans √™tre explicitement programm√©s pour chaque t√¢che. Il est particuli√®rement utile pour :
- **Pr√©dire** des valeurs futures (prix, rendements agricoles)
- **Classifier** des √©l√©ments (spam/non-spam, maladie/sant√©)
- **D√©tecter** des anomalies (fraude, pannes)
- **Recommander** des actions (politiques publiques optimales)

### Types de Machine Learning
1. **Supervis√©** : Apprendre avec des exemples √©tiquet√©s
   - R√©gression (pr√©dire des valeurs continues)
   - Classification (pr√©dire des cat√©gories)

2. **Non-supervis√©** : D√©couvrir des patterns sans √©tiquettes
   - Clustering (groupement)
   - R√©duction de dimension

3. **Par renforcement** : Apprendre par essai-erreur avec r√©compenses

## üîß Installation et Configuration MLJ.jl

### Packages n√©cessaires
```julia
using Pkg

# √âcosyst√®me MLJ
Pkg.add("MLJ")
Pkg.add("MLJBase")
Pkg.add("MLJModels")

# Mod√®les sp√©cifiques
Pkg.add("DecisionTree")      # Arbres de d√©cision
Pkg.add("GLM")              # Mod√®les lin√©aires g√©n√©ralis√©s
Pkg.add("MLJLinearModels")  # R√©gression lin√©aire
Pkg.add("MLJDecisionTreeInterface")

# Utilitaires
Pkg.add("StatsBase")
Pkg.add("Statistics")
Pkg.add("Random")

using MLJ
using DataFrames
using Statistics
using StatsBase
using Random
using Plots
```

### Interface MLJ.jl
```julia
# Exploration des mod√®les disponibles
models()

# Recherche de mod√®les par t√¢che
models(matching(X, y)) # X: features, y: target

# Mod√®les pour la r√©gression
models(matching(X, y)) |> DataFrame |> 
    x -> filter(row -> row.prediction_type == :deterministic, x)
```

## üìà Statistiques Descriptives et Inf√©rentielles

### Analyse descriptive avec StatsBase
```julia
# Donn√©es d'exemple : rendements agricoles par r√©gion
rendements_mil = [0.8, 1.2, 0.9, 1.5, 1.1, 0.7, 1.3, 1.0, 0.9, 1.4]
regions = ["Cascades", "Centre", "Est", "Hauts-Bassins", "Nord", 
           "Sahel", "Sud-Ouest", "Boucle du Mouhoun", "Centre-Nord", "Plateau Central"]

# Statistiques descriptives
println("Statistiques des rendements (tonnes/ha):")
println("Moyenne: $(round(mean(rendements_mil), digits=2))")
println("M√©diane: $(round(median(rendements_mil), digits=2))")
println("√âcart-type: $(round(std(rendements_mil), digits=2))")
println("Variance: $(round(var(rendements_mil), digits=2))")
println("√âtendue: $(round(maximum(rendements_mil) - minimum(rendements_mil), digits=2))")

# Quantiles
quantiles = quantile(rendements_mil, [0.25, 0.5, 0.75])
println("Q1: $(quantiles[1]), Q2: $(quantiles[2]), Q3: $(quantiles[3])")
```

### Tests statistiques
```julia
using HypothesisTests

# Test de normalit√© (Shapiro-Wilk)
# shapiro_test = ShapiroWilkTest(rendements_mil)

# Test t pour comparaison de moyennes
# Exemple: comparer rendements Nord vs Sud
rendements_nord = [0.7, 0.8, 0.9, 0.6, 0.8]
rendements_sud = [1.2, 1.4, 1.3, 1.5, 1.1]

# Test t de Student
# t_test = UnequalVarianceTTest(rendements_nord, rendements_sud)
# println("p-value: $(pvalue(t_test))")
```

## üèóÔ∏è Pr√©paration des Donn√©es pour ML

### Cr√©ation d'un dataset agricole
```julia
# Dataset pour pr√©diction de rendements agricoles
Random.seed!(123)

n_observations = 200
df_agri = DataFrame(
    # Variables m√©t√©orologiques
    precipitation_mm = rand(300:1200, n_observations),
    temperature_moy = rand(25.0:30.0, n_observations),
    humidite_pct = rand(40:80, n_observations),
    
    # Variables sol et pratiques
    qualite_sol = rand(["Pauvre", "Moyenne", "Bonne"], n_observations),
    irrigation = rand([0, 1], n_observations),  # 0=non, 1=oui
    engrais_kg_ha = rand(0:100, n_observations),
    semences_ameliorees = rand([0, 1], n_observations),
    
    # Variables socio-√©conomiques
    experience_agriculteur = rand(1:40, n_observations),
    superficie_hectares = rand(0.5:10.0, n_observations),
    acces_credit = rand([0, 1], n_observations),
    
    # Variable cible : rendement (tonnes/hectare)
    rendement_tonnes_ha = zeros(n_observations)
)

# G√©n√©ration r√©aliste de la variable cible
for i in 1:n_observations
    base_rendement = 0.6
    
    # Impact de la m√©t√©o
    if df_agri.precipitation_mm[i] > 800
        base_rendement += 0.3
    elseif df_agri.precipitation_mm[i] < 500
        base_rendement -= 0.2
    end
    
    # Impact de la temp√©rature
    if df_agri.temperature_moy[i] > 28
        base_rendement -= 0.1
    end
    
    # Impact des pratiques
    if df_agri.irrigation[i] == 1
        base_rendement += 0.4
    end
    
    if df_agri.engrais_kg_ha[i] > 50
        base_rendement += 0.3
    end
    
    if df_agri.semences_ameliorees[i] == 1
        base_rendement += 0.2
    end
    
    # Impact qualit√© sol
    if df_agri.qualite_sol[i] == "Bonne"
        base_rendement += 0.3
    elseif df_agri.qualite_sol[i] == "Pauvre"
        base_rendement -= 0.2
    end
    
    # Impact exp√©rience
    base_rendement += df_agri.experience_agriculteur[i] * 0.01
    
    # Bruit al√©atoire
    base_rendement += randn() * 0.1
    
    df_agri.rendement_tonnes_ha[i] = max(0.1, base_rendement)
end

println("Dataset agricole cr√©√© : $(nrow(df_agri)) observations")
println("Rendement moyen : $(round(mean(df_agri.rendement_tonnes_ha), digits=2)) t/ha")
```

### Encodage des variables cat√©gorielles
```julia
# Conversion des variables cat√©gorielles
function encoder_variables_categoriques!(df::DataFrame)
    # One-hot encoding pour qualit√© du sol
    df.sol_bon = (df.qualite_sol .== "Bonne") * 1
    df.sol_moyen = (df.qualite_sol .== "Moyenne") * 1
    df.sol_pauvre = (df.qualite_sol .== "Pauvre") * 1
    
    # Supprimer la colonne originale
    select!(df, Not(:qualite_sol))
    
    return df
end

encoder_variables_categoriques!(df_agri)
```

## üìä R√©gression Lin√©aire

### R√©gression lin√©aire simple
```julia
# Relation entre pr√©cipitations et rendement
X_simple = select(df_agri, :precipitation_mm)
y_simple = df_agri.rendement_tonnes_ha

# Chargement du mod√®le
LinearRegressor = @load LinearRegressor pkg=MLJLinearModels

# Cr√©ation de l'instance du mod√®le
model_simple = LinearRegressor()

# Ajustement du mod√®le
machine_simple = machine(model_simple, X_simple, y_simple)
fit!(machine_simple)

# Pr√©dictions
y_pred_simple = predict(machine_simple, X_simple)

# Visualisation
scatter(X_simple.precipitation_mm, y_simple, 
        title="Relation Pr√©cipitations-Rendement",
        xlabel="Pr√©cipitations (mm)", ylabel="Rendement (t/ha)",
        label="Observations", alpha=0.6)

plot!(X_simple.precipitation_mm, y_pred_simple, 
      label="R√©gression lin√©aire", linewidth=2, color=:red)
```

### R√©gression lin√©aire multiple
```julia
# S√©lection des variables explicatives
X_multiple = select(df_agri, Not([:rendement_tonnes_ha]))
y_multiple = df_agri.rendement_tonnes_ha

# Division train/test
train_indices, test_indices = partition(eachindex(y_multiple), 0.7, shuffle=true, rng=123)

X_train = X_multiple[train_indices, :]
X_test = X_multiple[test_indices, :]
y_train = y_multiple[train_indices]
y_test = y_multiple[test_indices]

# Mod√®le de r√©gression multiple
model_multiple = LinearRegressor()
machine_multiple = machine(model_multiple, X_train, y_train)
fit!(machine_multiple)

# Pr√©dictions
y_pred_train = predict(machine_multiple, X_train)
y_pred_test = predict(machine_multiple, X_test)

# √âvaluation
train_rmse = rmse(y_pred_train, y_train)
test_rmse = rmse(y_pred_test, y_test)
train_r2 = rsq(y_pred_train, y_train)
test_r2 = rsq(y_pred_test, y_test)

println("Performance du mod√®le :")
println("RMSE train: $(round(train_rmse, digits=3))")
println("RMSE test: $(round(test_rmse, digits=3))")
println("R¬≤ train: $(round(train_r2, digits=3))")
println("R¬≤ test: $(round(test_r2, digits=3))")
```

### Analyse des coefficients
```julia
# Extraction des coefficients (si disponible)
fitted_params = fitted_params(machine_multiple)
println("Coefficients du mod√®le :")
if haskey(fitted_params, :coefs)
    coefficients = fitted_params.coefs
    feature_names = names(X_train)
    
    for (i, coef) in enumerate(coefficients)
        println("$(feature_names[i]): $(round(coef, digits=4))")
    end
end
```

## üéØ Classification

### Dataset de classification : Adoption de technologies agricoles
```julia
# Cr√©ation d'un dataset pour classification binaire
Random.seed!(456)

n_farmers = 300
df_adoption = DataFrame(
    age = rand(20:70, n_farmers),
    education_annees = rand(0:15, n_farmers),
    superficie_ha = rand(0.5:20.0, n_farmers),
    revenus_annuels = rand(500000:5000000, n_farmers),
    acces_credit = rand([0, 1], n_farmers),
    distance_ville_km = rand(5:100, n_farmers),
    groupe_agriculteur = rand([0, 1], n_farmers),
    formation_recue = rand([0, 1], n_farmers)
)

# Variable cible : adoption de nouvelles vari√©t√©s (0=non, 1=oui)
df_adoption.adoption = zeros(Int, n_farmers)

for i in 1:n_farmers
    prob_adoption = 0.2  # probabilit√© de base
    
    # Facteurs positifs
    if df_adoption.education_annees[i] > 8
        prob_adoption += 0.3
    end
    
    if df_adoption.acces_credit[i] == 1
        prob_adoption += 0.25
    end
    
    if df_adoption.formation_recue[i] == 1
        prob_adoption += 0.35
    end
    
    if df_adoption.groupe_agriculteur[i] == 1
        prob_adoption += 0.2
    end
    
    # Facteurs n√©gatifs
    if df_adoption.age[i] > 50
        prob_adoption -= 0.15
    end
    
    if df_adoption.distance_ville_km[i] > 50
        prob_adoption -= 0.1
    end
    
    # D√©cision finale
    df_adoption.adoption[i] = rand() < prob_adoption ? 1 : 0
end

println("Dataset adoption cr√©√© :")
println("Taux d'adoption : $(round(mean(df_adoption.adoption) * 100, digits=1))%")
```

### Classification avec arbre de d√©cision
```julia
# Pr√©paration des donn√©es
X_classif = select(df_adoption, Not(:adoption))
y_classif = categorical(df_adoption.adoption)

# Division train/test
train_idx, test_idx = partition(eachindex(y_classif), 0.7, shuffle=true, rng=789)

X_train_c = X_classif[train_idx, :]
X_test_c = X_classif[test_idx, :]
y_train_c = y_classif[train_idx]
y_test_c = y_classif[test_idx]

# Mod√®le d'arbre de d√©cision
DecisionTreeClassifier = @load DecisionTreeClassifier pkg=DecisionTree

tree_model = DecisionTreeClassifier(max_depth=5, min_samples_leaf=10)
tree_machine = machine(tree_model, X_train_c, y_train_c)
fit!(tree_machine)

# Pr√©dictions
y_pred_tree = predict_mode(tree_machine, X_test_c)

# Matrice de confusion
conf_matrix = confusion_matrix(y_pred_tree, y_test_c)
println("Matrice de confusion :")
println(conf_matrix)

# M√©triques de performance
accuracy = mean(y_pred_tree .== y_test_c)
println("Accuracy : $(round(accuracy, digits=3))")
```

### Classification avec r√©gression logistique
```julia
# Mod√®le de r√©gression logistique
LogisticClassifier = @load LogisticClassifier pkg=MLJLinearModels

logistic_model = LogisticClassifier()
logistic_machine = machine(logistic_model, X_train_c, y_train_c)
fit!(logistic_machine)

# Pr√©dictions probabilistes
y_prob_logistic = predict(logistic_machine, X_test_c)
y_pred_logistic = predict_mode(logistic_machine, X_test_c)

# √âvaluation
accuracy_logistic = mean(y_pred_logistic .== y_test_c)
println("Accuracy r√©gression logistique : $(round(accuracy_logistic, digits=3))")

# Courbe ROC (si applicable)
# roc_curve = roc(y_prob_logistic, y_test_c)
```

## üîÑ Validation Crois√©e et Optimisation

### Validation crois√©e
```julia
# Validation crois√©e k-fold
function cross_validation_score(model, X, y, cv=5)
    scores = Float64[]
    
    # Stratification pour maintenir la proportion des classes
    cv_indices = MLJ.CV(nfolds=cv, shuffle=true, rng=123)
    
    for (train_idx, val_idx) in cv_indices(1:nrows(X), y)
        X_train_cv = selectrows(X, train_idx)
        X_val_cv = selectrows(X, val_idx)
        y_train_cv = y[train_idx]
        y_val_cv = y[val_idx]
        
        # Entra√Ænement
        mach = machine(model, X_train_cv, y_train_cv)
        fit!(mach, verbosity=0)
        
        # Pr√©diction et √©valuation
        y_pred = predict_mode(mach, X_val_cv)
        score = mean(y_pred .== y_val_cv)
        push!(scores, score)
    end
    
    return scores
end

# Test avec diff√©rents mod√®les
models_to_test = [
    ("Decision Tree", DecisionTreeClassifier(max_depth=5)),
    ("Logistic Regression", LogisticClassifier())
]

for (name, model) in models_to_test
    scores = cross_validation_score(model, X_classif, y_classif)
    println("$name - CV Score: $(round(mean(scores), digits=3)) ¬± $(round(std(scores), digits=3))")
end
```

### Optimisation des hyperparam√®tres
```julia
# Grid search pour optimiser les hyperparam√®tres de l'arbre de d√©cision
using MLJTuning

# D√©finition de la grille de param√®tres
r_max_depth = range(tree_model, :max_depth, values=[3, 5, 7, 10])
r_min_samples = range(tree_model, :min_samples_leaf, values=[5, 10, 20])

# Tuning avec grid search
tuning = Grid(resolution=8)
tuned_tree = TunedModel(
    model=tree_model,
    tuning=tuning,
    resampling=CV(nfolds=5),
    ranges=[r_max_depth, r_min_samples],
    measure=accuracy
)

# Entra√Ænement du mod√®le optimis√©
tuned_machine = machine(tuned_tree, X_train_c, y_train_c)
fit!(tuned_machine)

# Meilleurs param√®tres
best_model = fitted_params(tuned_machine).best_model
println("Meilleurs param√®tres :")
println("max_depth: $(best_model.max_depth)")
println("min_samples_leaf: $(best_model.min_samples_leaf)")
```

## üìä √âvaluation et M√©triques

### M√©triques pour la r√©gression
```julia
function evaluer_regression(y_true, y_pred)
    mae_val = mean(abs.(y_true .- y_pred))
    mse_val = mean((y_true .- y_pred).^2)
    rmse_val = sqrt(mse_val)
    r2_val = 1 - sum((y_true .- y_pred).^2) / sum((y_true .- mean(y_true)).^2)
    
    println("M√©triques de r√©gression :")
    println("MAE (Mean Absolute Error): $(round(mae_val, digits=3))")
    println("MSE (Mean Squared Error): $(round(mse_val, digits=3))")
    println("RMSE (Root Mean Squared Error): $(round(rmse_val, digits=3))")
    println("R¬≤ (Coefficient de d√©termination): $(round(r2_val, digits=3))")
    
    return (mae=mae_val, mse=mse_val, rmse=rmse_val, r2=r2_val)
end

# √âvaluation du mod√®le de rendement
metrics = evaluer_regression(y_test, y_pred_test)
```

### M√©triques pour la classification
```julia
function evaluer_classification(y_true, y_pred)
    # Conversion en vecteurs si n√©cessaire
    y_true_vec = MLJ.int.(y_true)
    y_pred_vec = MLJ.int.(y_pred)
    
    # Matrice de confusion
    conf_mat = confusion_matrix(y_pred, y_true)
    
    # Calculs manuels des m√©triques
    tp = sum((y_true_vec .== 1) .& (y_pred_vec .== 1))
    tn = sum((y_true_vec .== 0) .& (y_pred_vec .== 0))
    fp = sum((y_true_vec .== 0) .& (y_pred_vec .== 1))
    fn = sum((y_true_vec .== 1) .& (y_pred_vec .== 0))
    
    accuracy = (tp + tn) / (tp + tn + fp + fn)
    precision = tp / (tp + fp)
    recall = tp / (tp + fn)
    f1 = 2 * (precision * recall) / (precision + recall)
    
    println("M√©triques de classification :")
    println("Accuracy (Exactitude): $(round(accuracy, digits=3))")
    println("Precision (Pr√©cision): $(round(precision, digits=3))")
    println("Recall (Rappel): $(round(recall, digits=3))")
    println("F1-Score: $(round(f1, digits=3))")
    
    return (accuracy=accuracy, precision=precision, recall=recall, f1=f1)
end

# √âvaluation du mod√®le d'adoption
classification_metrics = evaluer_classification(y_test_c, y_pred_logistic)
```

## üéØ Applications Pratiques au Burkina Faso

### Mod√®le de pr√©diction de s√©curit√© alimentaire
```julia
# Dataset pour pr√©diction de l'ins√©curit√© alimentaire
function create_food_security_data()
    Random.seed!(999)
    n_communes = 150
    
    DataFrame(
        # Variables m√©t√©orologiques
        pluviometrie_cumul = rand(250:1000, n_communes),
        temp_moyenne = rand(26:32, n_communes),
        jours_secheresse = rand(0:60, n_communes),
        
        # Variables agricoles
        superficie_cultivee = rand(1000:50000, n_communes),
        rendement_moyen = rand(0.4:1.8, n_communes),
        stocks_cereales = rand(500:15000, n_communes),
        
        # Variables socio-√©conomiques
        prix_cereales_fcfa = rand(180:350, n_communes),
        acces_marche_km = rand(5:80, n_communes),
        population_totale = rand(15000:200000, n_communes),
        taux_pauvrete = rand(30:80, n_communes),
        
        # Variables infrastructure
        routes_praticables = rand([0, 1], n_communes),
        centres_stockage = rand(0:5, n_communes),
        
        # Variable cible : niveau s√©curit√© alimentaire (1=s√©curis√©, 0=ins√©curis√©)
        securite_alimentaire = zeros(Int, n_communes)
    )
end

df_food = create_food_security_data()

# G√©n√©ration r√©aliste de la variable cible
for i in 1:nrow(df_food)
    score_securite = 0.5
    
    # Impact pluviom√©trie
    if df_food.pluviometrie_cumul[i] > 600
        score_securite += 0.3
    elseif df_food.pluviometrie_cumul[i] < 400
        score_securite -= 0.4
    end
    
    # Impact rendement
    if df_food.rendement_moyen[i] > 1.2
        score_securite += 0.3
    elseif df_food.rendement_moyen[i] < 0.8
        score_securite -= 0.3
    end
    
    # Impact prix
    if df_food.prix_cereales_fcfa[i] > 280
        score_securite -= 0.2
    end
    
    # Impact pauvret√©
    if df_food.taux_pauvrete[i] > 60
        score_securite -= 0.25
    end
    
    # Impact acc√®s march√©
    if df_food.acces_marche_km[i] > 40
        score_securite -= 0.15
    end
    
    df_food.securite_alimentaire[i] = rand() < score_securite ? 1 : 0
end

println("Dataset s√©curit√© alimentaire :")
println("Taux de s√©curit√© alimentaire : $(round(mean(df_food.securite_alimentaire) * 100, digits=1))%")
```

### Syst√®me d'alerte pr√©coce
```julia
# Mod√®le d'alerte pr√©coce pour l'ins√©curit√© alimentaire
X_food = select(df_food, Not(:securite_alimentaire))
y_food = categorical(df_food.securite_alimentaire)

# Entra√Ænement du mod√®le
early_warning_model = LogisticClassifier()
ew_machine = machine(early_warning_model, X_food, y_food)
fit!(ew_machine)

# Pr√©dictions probabilistes pour scoring de risque
risk_probabilities = predict(ew_machine, X_food)

# Cr√©ation d'un syst√®me de scoring
df_food.risk_score = [prob.prob_given_ref[1] for prob in risk_probabilities]  # Probabilit√© d'ins√©curit√©
df_food.risk_level = ifelse.(df_food.risk_score .> 0.7, "√âlev√©",
                     ifelse.(df_food.risk_score .> 0.4, "Mod√©r√©", "Faible"))

println("Distribution des niveaux de risque :")
println(countmap(df_food.risk_level))
```

## üîÆ Clustering et Analyse Non-supervis√©e

### Clustering des r√©gions par profil de d√©veloppement
```julia
using Clustering

# Dataset pour clustering des r√©gions
df_regions = DataFrame(
    region = ["Centre", "Hauts-Bassins", "Nord", "Sahel", "Est", "Sud-Ouest", 
              "Cascades", "Centre-Est", "Boucle du Mouhoun", "Centre-Nord"],
    pib_par_hab = [750000, 580000, 320000, 280000, 420000, 480000, 
                   520000, 450000, 540000, 380000],
    taux_alphabetisation = [78, 53, 29, 22, 36, 35, 
                           50, 49, 41, 32],
    acces_eau = [90, 68, 46, 34, 52, 50, 
                 68, 61, 59, 47],
    production_agricole = [485, 624, 723, 484, 557, 398, 
                          315, 452, 685, 398]
)

# Normalisation des donn√©es
using StatsBase
data_matrix = Matrix(select(df_regions, Not(:region)))
data_normalized = StatsBase.standardize(StatsBase.ZScoreTransform, data_matrix, dims=1)

# K-means clustering
k = 3  # Nombre de clusters
kmeans_result = kmeans(data_normalized', k)

# Ajout des clusters au DataFrame
df_regions.cluster = kmeans_result.assignments

println("Clustering des r√©gions :")
for cluster_id in 1:k
    regions_cluster = df_regions[df_regions.cluster .== cluster_id, :region]
    println("Cluster $cluster_id: $(join(regions_cluster, ", "))")
end
```

## üìä Visualisation des R√©sultats ML

### Graphiques de performance
```julia
# Graphique r√©sidus vs pr√©dictions pour r√©gression
function plot_residuals(y_true, y_pred)
    residuals = y_true .- y_pred
    
    p1 = scatter(y_pred, residuals,
        title="R√©sidus vs Pr√©dictions",
        xlabel="Valeurs pr√©dites", ylabel="R√©sidus",
        alpha=0.6)
    hline!([0], linestyle=:dash, color=:red)
    
    p2 = histogram(residuals,
        title="Distribution des R√©sidus",
        xlabel="R√©sidus", ylabel="Fr√©quence",
        alpha=0.7)
    
    plot(p1, p2, layout=(1,2), size=(800, 300))
end

plot_residuals(y_test, y_pred_test)
```

### Importance des variables
```julia
# Pour les mod√®les qui le supportent
function plot_feature_importance(model, feature_names)
    # Cette fonction d√©pend du type de mod√®le
    # Pour les arbres de d√©cision, on peut extraire l'importance
    println("Importance des variables (impl√©mentation sp√©cifique au mod√®le)")
end
```

## üéì Bonnes Pratiques ML

### Pipeline de machine learning
```julia
# Pipeline type pour un projet ML
function ml_pipeline(X, y; test_size=0.2, random_state=42)
    println("üîÑ Pipeline Machine Learning")
    
    # 1. Division des donn√©es
    train_idx, test_idx = partition(eachindex(y), 1-test_size, shuffle=true, rng=random_state)
    X_train, X_test = X[train_idx, :], X[test_idx, :]
    y_train, y_test = y[train_idx], y[test_idx]
    
    println("‚úÖ Donn√©es divis√©es: $(length(train_idx)) train, $(length(test_idx)) test")
    
    # 2. Pr√©processing (si n√©cessaire)
    # Normalisation, encodage, etc.
    
    # 3. Entra√Ænement de plusieurs mod√®les
    models = Dict(
        "LinearRegressor" => LinearRegressor(),
        # Ajouter d'autres mod√®les selon le type de probl√®me
    )
    
    results = Dict()
    
    for (name, model) in models
        println("üöÄ Entra√Ænement $name...")
        
        # Validation crois√©e
        cv_scores = cross_validation_score(model, X_train, y_train)
        
        # Entra√Ænement final
        mach = machine(model, X_train, y_train)
        fit!(mach, verbosity=0)
        
        # √âvaluation
        y_pred = predict(mach, X_test)
        test_score = rmse(y_pred, y_test)  # ou autre m√©trique selon le probl√®me
        
        results[name] = Dict(
            "cv_mean" => mean(cv_scores),
            "cv_std" => std(cv_scores),
            "test_score" => test_score,
            "model" => mach
        )
        
        println("   CV: $(round(mean(cv_scores), digits=3)) ¬± $(round(std(cv_scores), digits=3))")
        println("   Test: $(round(test_score, digits=3))")
    end
    
    # 4. S√©lection du meilleur mod√®le
    best_model_name = argmin([results[name]["test_score"] for name in keys(results)])
    println("üèÜ Meilleur mod√®le: $best_model_name")
    
    return results
end
```

### Validation et robustesse
```julia
# Tests de robustesse
function test_model_robustness(model, X, y)
    println("üîç Tests de robustesse")
    
    # Test avec diff√©rentes graines al√©atoires
    scores = []
    for seed in 1:10
        train_idx, test_idx = partition(eachindex(y), 0.8, shuffle=true, rng=seed)
        X_train, X_test = X[train_idx, :], X[test_idx, :]
        y_train, y_test = y[train_idx], y[test_idx]
        
        mach = machine(model, X_train, y_train)
        fit!(mach, verbosity=0)
        y_pred = predict(mach, X_test)
        
        score = rmse(y_pred, y_test)
        push!(scores, score)
    end
    
    println("Score moyen: $(round(mean(scores), digits=3))")
    println("√âcart-type: $(round(std(scores), digits=3))")
    println("Coefficient de variation: $(round(std(scores)/mean(scores)*100, digits=1))%")
    
    return scores
end
```

## üèÜ Points Cl√©s √† Retenir

### Concepts Fondamentaux
1. **Overfitting vs Underfitting** : √âquilibre entre complexit√© et g√©n√©ralisation
2. **Bias-Variance Tradeoff** : Compromis entre biais et variance
3. **Validation crois√©e** : Estimation robuste des performances
4. **M√©triques appropri√©es** : Choisir selon le probl√®me et le contexte

### Workflow MLJ.jl
1. **Chargement** : `@load ModelName pkg=PackageName`
2. **Instanciation** : `model = ModelName(params...)`
3. **Machine** : `mach = machine(model, X, y)`
4. **Entra√Ænement** : `fit!(mach)`
5. **Pr√©diction** : `predict(mach, X_new)`

### Applications au Burkina Faso
- **Agriculture** : Pr√©diction de rendements, optimisation des pratiques
- **Sant√©** : D√©tection pr√©coce d'√©pid√©mies, allocation de ressources
- **√âconomie** : √âvaluation de politiques, ciblage des interventions
- **Environnement** : Surveillance d√©forestation, gestion ressources

### √âthique et Responsabilit√©
- **Biais dans les donn√©es** : Attention aux biais soci√©taux
- **Interpr√©tabilit√©** : Mod√®les explicables pour les d√©cideurs
- **Impact social** : Consid√©rer les cons√©quences des pr√©dictions
- **Transparence** : Documenter les limites et incertitudes

Dans les prochaines sessions pratiques, nous appliquerons ces concepts √† des cas d'usage concrets : pr√©diction de rendements agricoles et classification de messages en langues locales !
# Exercice 2 : DataFrames.jl - Manipulation AvancÃ©e de DonnÃ©es
# Module 3 : Apprentissage Automatique avec Julia
# DurÃ©e : 60 minutes

# ðŸ“š AVANT DE COMMENCER
# Lisez le rÃ©sumÃ© d'apprentissage : resume_02_dataframes.md
# DÃ©couvrez pourquoi DataFrames.jl rÃ©volutionne l'analyse de donnÃ©es !

println("ðŸ“š Consultez le rÃ©sumÃ© : modules/module3-ml/resume_02_dataframes.md")
println("Appuyez sur EntrÃ©e quand vous Ãªtes prÃªt Ã  manipuler des donnÃ©es comme un pro...")
readline()

println("ðŸ“Š DataFrames.jl : Manipulation AvancÃ©e de DonnÃ©es")
println("="^60)

# Installation et importation des paquets
using DataFrames, CSV, Statistics, StatsBase, Dates
using Chain, DataFramesMeta
using Random
Random.seed!(42)

# Configuration d'affichage
ENV["COLUMNS"] = 120  # Pour un meilleur affichage des DataFrames

# Partie 1 : CrÃ©ation et Exploration de DataFrames
println("ðŸ—ï¸ Partie 1 : CrÃ©ation et Exploration de DataFrames")

# CrÃ©ation manuelle d'un DataFrame
println("CrÃ©ation d'un DataFrame de dÃ©monstration...")
df_ventes = DataFrame(
    produit=["Ordinateur", "Tablet", "Smartphone", "Ã‰couteurs", "Clavier", "Souris"],
    prix=[1200.0, 600.0, 800.0, 150.0, 80.0, 25.0],
    quantitÃ©=[45, 120, 200, 300, 150, 400],
    catÃ©gorie=["Ã‰lectronique", "Ã‰lectronique", "Ã‰lectronique", "Audio", "Accessoires", "Accessoires"],
    date_vente=Date(2024, 1, 1):Month(1):Date(2024, 6, 1)
)

println("DataFrame de ventes initial :")
println(df_ventes)

# Exploration basique
println("\nInformations sur le DataFrame :")
println("  - Dimensions : $(size(df_ventes))")
println("  - Colonnes : $(names(df_ventes))")
println("  - Types : ")
for (nom, type) in zip(names(df_ventes), eltype.(eachcol(df_ventes)))
    println("    $nom : $type")
end

# Statistiques descriptives
println("\nStatistiques descriptives :")
println(describe(df_ventes))

# Partie 2 : Indexation et SÃ©lection AvancÃ©e
println("\nðŸŽ¯ Partie 2 : Indexation et SÃ©lection de DonnÃ©es")

println("SÃ©lection par colonnes :")
println("Prix et quantitÃ©s :")
println(select(df_ventes, [:prix, :quantitÃ©]))

println("\nSÃ©lection avec transformation :")
df_avec_ca = select(df_ventes,
    :produit,
    :prix,
    :quantitÃ©,
    :ca => ByRow((prix, qty) -> prix * qty) => :chiffre_affaires)
println(df_avec_ca)

# Filtrage avancÃ©
println("\nFiltrage des donnÃ©es :")
println("Produits avec prix > 100â‚¬ :")
produits_chers = filter(row -> row.prix > 100, df_ventes)
println(produits_chers)

println("\nProduits Ã©lectroniques avec quantitÃ© > 100 :")
Ã©lectronique_populaire = filter(row -> row.catÃ©gorie == "Ã‰lectronique" && row.quantitÃ© > 100, df_ventes)
println(Ã©lectronique_populaire)

# Partie 3 : Transformations et Mutations
println("\nðŸ”§ Partie 3 : Transformations et Calculs")

# Ajout de colonnes calculÃ©es
println("Ajout de colonnes calculÃ©es...")
df_enrichi = copy(df_ventes)

# Calcul du chiffre d'affaires
df_enrichi.chiffre_affaires = df_enrichi.prix .* df_enrichi.quantitÃ©

# Calcul de catÃ©gories de prix
df_enrichi.gamme = map(prix -> prix < 100 ? "Ã‰conomique" :
                               prix < 500 ? "Milieu de gamme" : "Premium",
    df_enrichi.prix)

# Trimestre de vente
df_enrichi.trimestre = map(date -> "T$(quarter(date))", df_enrichi.date_vente)

println("DataFrame enrichi :")
println(df_enrichi)

# Partie 4 : Groupement et AgrÃ©gation
println("\nðŸ“ˆ Partie 4 : Analyse par Groupes")

println("Analyse par catÃ©gorie :")
analyse_catÃ©gorie = combine(groupby(df_enrichi, :catÃ©gorie)) do group
    DataFrame(
        nb_produits=nrow(group),
        ca_total=sum(group.chiffre_affaires),
        prix_moyen=mean(group.prix),
        quantitÃ©_totale=sum(group.quantitÃ©)
    )
end
println(analyse_catÃ©gorie)

println("\nAnalyse par gamme de prix :")
analyse_gamme = combine(groupby(df_enrichi, :gamme)) do group
    DataFrame(
        produits=nrow(group),
        ca_moyen=mean(group.chiffre_affaires),
        prix_min=minimum(group.prix),
        prix_max=maximum(group.prix)
    )
end
println(analyse_gamme)

# Analyse temporelle
println("\nAnalyse par trimestre :")
analyse_temporelle = combine(groupby(df_enrichi, :trimestre)) do group
    DataFrame(
        ca_trimestre=sum(group.chiffre_affaires),
        ventes_moyennes=mean(group.quantitÃ©),
        nb_rÃ©fÃ©rences=nrow(group)
    )
end
println(analyse_temporelle)

# Partie 5 : DataFramesMeta.jl - Syntaxe Moderne
println("\nâœ¨ Partie 5 : Syntaxe Moderne avec DataFramesMeta")

println("Utilisation de @select, @filter, @transform...")

# Syntaxe moderne Ã©quivalente
rÃ©sultat_moderne = @chain df_ventes begin
    @select(:produit, :prix, :quantitÃ©)
    @transform(:ca = :prix * :quantitÃ©)
    @filter(:ca > 10000)
    @orderby(-:ca)
end

println("Produits avec CA > 10000â‚¬ (syntaxe moderne) :")
println(rÃ©sultat_moderne)

# Pipeline complexe
analyse_complexe = @chain df_enrichi begin
    @filter(:catÃ©gorie != "Accessoires")
    @transform(:rentabilitÃ© = :chiffre_affaires / :quantitÃ©)
    @groupby(:catÃ©gorie)
    @combine(
        :ca_total = sum(:chiffre_affaires),
        :rentabilitÃ©_moyenne = mean(:rentabilitÃ©),
        :meilleur_produit = :produit[argmax(:chiffre_affaires)]
    )
end

println("\nAnalyse complexe par catÃ©gorie (hors accessoires) :")
println(analyse_complexe)

# Partie 6 : Jointures et Fusion de DonnÃ©es
println("\nðŸ”— Partie 6 : Jointures et Fusion de DataFrames")

# CrÃ©er un DataFrame de stock
df_stock = DataFrame(
    produit=["Ordinateur", "Tablet", "Smartphone", "Ã‰couteurs", "Webcam", "Imprimante"],
    stock_disponible=[12, 45, 67, 89, 34, 23],
    fournisseur=["Faso Tech", "Faso Tech", "Burkina Mobile", "Audio Sahel", "Faso Tech", "Impression BF"],
    dÃ©lai_livraison=[7, 5, 3, 2, 10, 14]
)

println("DataFrame de stock :")
println(df_stock)

# Inner join
println("\nJointure interne (produits prÃ©sents dans les deux tables) :")
ventes_avec_stock = innerjoin(df_enrichi, df_stock, on=:produit)
println(select(ventes_avec_stock, [:produit, :quantitÃ©, :stock_disponible, :fournisseur]))

# Left join
println("\nJointure gauche (tous les produits de ventes + stock si disponible) :")
ventes_complÃ¨tes = leftjoin(df_enrichi, df_stock, on=:produit)
println(select(ventes_complÃ¨tes, [:produit, :quantitÃ©, :stock_disponible]))

# Outer join
println("\nJointure externe (tous les produits des deux tables) :")
inventaire_complet = outerjoin(df_enrichi, df_stock, on=:produit)
println(select(inventaire_complet, [:produit, :quantitÃ©, :stock_disponible]))

# Partie 7 : Gestion des DonnÃ©es Manquantes
println("\nðŸ” Partie 7 : Gestion des Valeurs Manquantes")

# CrÃ©er des donnÃ©es avec valeurs manquantes
df_avec_missing = DataFrame(
    nom=["Aminata", "Boukary", "Clarisse", "Daouda", "Eveline"],
    Ã¢ge=[25, missing, 35, 28, missing],
    salaire=[450000, 520000, missing, 480000, 550000],  # Salaires en FCFA
    dÃ©partement=["Informatique", "RH", "Informatique", missing, "Finance"]
)

println("DataFrame avec valeurs manquantes :")
println(df_avec_missing)

# Identifier les valeurs manquantes
println("\nAnalyse des valeurs manquantes :")
for col in names(df_avec_missing)
    nb_missing = count(ismissing, df_avec_missing[!, col])
    println("  $col : $nb_missing valeurs manquantes")
end

# Filtrer les lignes complÃ¨tes
println("\nLignes sans valeurs manquantes :")
lignes_complÃ¨tes = dropmissing(df_avec_missing)
println(lignes_complÃ¨tes)

# Remplacer les valeurs manquantes
println("\nRemplacement des valeurs manquantes :")
df_sans_missing = copy(df_avec_missing)

# Remplacer l'Ã¢ge manquant par la moyenne
Ã¢ge_moyen = mean(skipmissing(df_sans_missing.Ã¢ge))
df_sans_missing.Ã¢ge = coalesce.(df_sans_missing.Ã¢ge, round(Int, Ã¢ge_moyen))

# Remplacer le salaire manquant par la mÃ©diane
salaire_mÃ©dian = median(skipmissing(df_sans_missing.salaire))
df_sans_missing.salaire = coalesce.(df_sans_missing.salaire, salaire_mÃ©dian)

# Remplacer dÃ©partement manquant par "Non spÃ©cifiÃ©"
df_sans_missing.dÃ©partement = coalesce.(df_sans_missing.dÃ©partement, "Non spÃ©cifiÃ©")

println(df_sans_missing)

# Partie 8 : Reshaping et Pivot
println("\nðŸ”„ Partie 8 : Transformation de Structure (Reshape/Pivot)")

# CrÃ©er un DataFrame de ventes trimestrielles
df_trimestres = DataFrame(
    rÃ©gion=["Nord", "Sud", "Est", "Ouest", "Nord", "Sud", "Est", "Ouest"],
    trimestre=["T1", "T1", "T1", "T1", "T2", "T2", "T2", "T2"],
    ventes=[120, 95, 110, 85, 140, 105, 125, 92]
)

println("DonnÃ©es de ventes par rÃ©gion et trimestre (format long) :")
println(df_trimestres)

# Pivot wider (de long vers large)
println("\nTransformation en format large (pivot) :")
df_pivot = unstack(df_trimestres, :trimestre, :ventes)
println(df_pivot)

# Pivot longer (de large vers long)
println("\nRetour au format long :")
df_long = stack(df_pivot, [:T1, :T2], variable_name=:trimestre, value_name=:ventes)
println(sort(df_long, [:rÃ©gion, :trimestre]))

# Partie 9 : OpÃ©rations sur ChaÃ®nes et Dates
println("\nðŸ“… Partie 9 : Manipulation de ChaÃ®nes et Dates")

# CrÃ©er un dataset avec dates et chaÃ®nes
df_logs = DataFrame(
    timestamp=DateTime(2024, 1, 1):Hour(6):DateTime(2024, 1, 3),
    user_agent=["Chrome/121.0 (Windows)", "Firefox/122.0 (Mac)", "Safari/17.0 (iOS)",
        "Chrome/121.0 (Android)", "Edge/120.0 (Windows)", "Firefox/122.0 (Linux)",
        "Chrome/121.0 (Windows)", "Safari/17.0 (Mac)", "Chrome/121.0 (Windows)"],
    status_code=[200, 404, 200, 500, 200, 403, 200, 200, 301]
)

println("Logs d'accÃ¨s web :")
println(df_logs)

# Extraction d'informations des chaÃ®nes
println("\nExtraction du navigateur et OS :")
df_logs_enrichi = @chain df_logs begin
    @transform(
        :navigateur = map(ua -> split(split(ua, "/")[1], " ")[1], :user_agent),
        :os = map(ua -> match(r"\(([^)]+)\)", ua).captures[1], :user_agent),
        :heure = hour.(:timestamp),
        :jour = dayname.(:timestamp)
    )
end

println(select(df_logs_enrichi, [:timestamp, :navigateur, :os, :status_code]))

# Analyse par navigateur et statut
analyse_logs = @chain df_logs_enrichi begin
    @groupby([:navigateur, :status_code])
    @combine(:count = length(:timestamp))
    @orderby(-:count)
end

println("\nAnalyse des logs par navigateur et code de statut :")
println(analyse_logs)

# Partie 10 : Performance et Optimisation
println("\nâš¡ Partie 10 : Optimisation de Performance")

# CrÃ©er un dataset plus grand pour les tests de performance
println("CrÃ©ation d'un grand dataset pour tests de performance...")
n = 100_000
df_large = DataFrame(
    id=1:n,
    groupe=rand(["A", "B", "C", "D"], n),
    valeur=rand(n),
    date=rand(Date(2023, 1, 1):Day(1):Date(2024, 12, 31), n)
)

println("Dataset crÃ©Ã© : $(nrow(df_large)) lignes")

# Test de performance : groupby vs indexation
println("\nTest de performance - Groupby :")
temps_groupby = @elapsed begin
    rÃ©sultat_groupby = combine(groupby(df_large, :groupe), :valeur => mean => :moyenne)
end
println("  Temps groupby : $(round(temps_groupby * 1000, digits=2))ms")
println("  RÃ©sultat : ", rÃ©sultat_groupby)

# Test de performance : filtrage
println("\nTest de performance - Filtrage :")
temps_filter = @elapsed begin
    rÃ©sultat_filter = filter(row -> row.valeur > 0.5, df_large)
end
println("  Temps filtrage : $(round(temps_filter * 1000, digits=2))ms")
println("  Lignes filtrÃ©es : $(nrow(rÃ©sultat_filter))")

# Optimisation mÃ©moire
println("\nOptimisation mÃ©moire - CatÃ©gories :")
df_optimisÃ© = copy(df_large)
df_optimisÃ©.groupe = categorical(df_optimisÃ©.groupe)

mÃ©moire_avant = Base.summarysize(df_large.groupe)
mÃ©moire_aprÃ¨s = Base.summarysize(df_optimisÃ©.groupe)

println("  MÃ©moire avant (String) : $(round(mÃ©moire_avant / 1024^2, digits=2)) MB")
println("  MÃ©moire aprÃ¨s (Categorical) : $(round(mÃ©moire_aprÃ¨s / 1024^2, digits=2)) MB")
println("  RÃ©duction : $(round((1 - mÃ©moire_aprÃ¨s/mÃ©moire_avant) * 100, digits=1))%")

# Partie 11 : Export et Sauvegarde
println("\nðŸ’¾ Partie 11 : Export et Persistance")

# Sauvegarder en CSV
println("Sauvegarde des analyses en CSV...")
try
    CSV.write("analyse_ventes.csv", df_enrichi)
    CSV.write("analyse_categories.csv", analyse_catÃ©gorie)
    println("âœ… Fichiers CSV sauvegardÃ©s :")
    println("  - analyse_ventes.csv")
    println("  - analyse_categories.csv")
catch e
    println("âš ï¸ Erreur lors de la sauvegarde : $e")
end

# CrÃ©er un rÃ©sumÃ© exÃ©cutif
rÃ©sumÃ©_exÃ©cutif = DataFrame(
    mÃ©trique=["CA Total", "Produit le plus vendu", "CatÃ©gorie dominante", "Trimestre le plus fort"],
    valeur=[
        string(round(sum(df_enrichi.chiffre_affaires), digits=0), "â‚¬"),
        df_enrichi.produit[argmax(df_enrichi.chiffre_affaires)],
        analyse_catÃ©gorie.catÃ©gorie[argmax(analyse_catÃ©gorie.ca_total)],
        analyse_temporelle.trimestre[argmax(analyse_temporelle.ca_trimestre)]
    ]
)

println("\nRÃ©sumÃ© exÃ©cutif :")
println(rÃ©sumÃ©_exÃ©cutif)

# Partie 12 : Pipeline ETL Complet
println("\nðŸ”„ Partie 12 : Pipeline ETL (Extract, Transform, Load)")

println("DÃ©monstration d'un pipeline ETL complet...")

function pipeline_etl(donnÃ©es_brutes)
    println("ðŸ” EXTRACT : Chargement des donnÃ©es...")
    df = copy(donnÃ©es_brutes)

    println("ðŸ”§ TRANSFORM : Nettoyage et enrichissement...")
    df_transformÃ© = @chain df begin
        # Nettoyage
        dropmissing(:produit)
        @filter(:prix > 0 && :quantitÃ© > 0)

        # Enrichissement
        @transform(
            :ca = :prix * :quantitÃ©,
            :prix_unitaire = round.(:prix, digits=2),
            :gamme = map(p -> p < 100 ? "Ã‰co" : p < 500 ? "Standard" : "Premium", :prix)
        )

        # Normalisation
        @transform(:produit = uppercase.(:produit))
    end

    println("ðŸ“Š LOAD : AgrÃ©gation finale...")
    rapport_final = @chain df_transformÃ© begin
        @groupby(:catÃ©gorie)
        @combine(
            :nb_produits = length(:produit),
            :ca_total = sum(:ca),
            :ca_moyen = mean(:ca),
            :meilleur_ca = maximum(:ca)
        )
        @orderby(-:ca_total)
    end

    return df_transformÃ©, rapport_final
end

# ExÃ©cution du pipeline
donnÃ©es_propres, rapport = pipeline_etl(df_ventes)

println("\nDonnÃ©es aprÃ¨s pipeline ETL :")
println(first(donnÃ©es_propres, 3))

println("\nRapport final :")
println(rapport)

# Bilan d'apprentissage
println("\nðŸ“ˆ BILAN D'APPRENTISSAGE")
println("="^70)
println("ðŸ“Š MAÃŽTRISE EXPERTE DE DATAFRAMES.JL !")
println("="^70)
println("âœ… CompÃ©tences de Data Scientist dÃ©veloppÃ©es :")
println("  ðŸ—ï¸ CrÃ©ation et manipulation de DataFrames complexes")
println("  ðŸŽ¯ SÃ©lection et filtrage de donnÃ©es avec conditions avancÃ©es")
println("  ðŸ”§ Transformations et calculs de colonnes dÃ©rivÃ©es")
println("  ðŸ“ˆ Groupements et agrÃ©gations pour analyses statistiques")
println("  âœ¨ Syntaxe moderne avec DataFramesMeta et chaÃ®nes de traitement")
println("  ðŸ”— Jointures multiples et fusion de sources de donnÃ©es")
println("  ðŸ” Gestion professionnelle des valeurs manquantes")
println("  ðŸ”„ Transformation de structure (pivot, reshape)")
println("  ðŸ“… Manipulation avancÃ©e de chaÃ®nes et dates")
println("  âš¡ Optimisation de performance pour big data")
println("  ðŸ’¾ Pipelines ETL complets avec export automatisÃ©")

println("\nðŸŒŸ BADGE DÃ‰BLOQUÃ‰ : 'Data Engineer Julia'")
println("Vous maÃ®trisez maintenant l'ensemble des outils pour manipuler")
println("et analyser des donnÃ©es comme un professionnel !")

println("\nðŸŽ¯ COMPÃ‰TENCES TRANSFÃ‰RABLES :")
println("  - Analyse de donnÃ©es business avec mÃ©triques KPI")
println("  - Nettoyage et prÃ©paration de datasets ML")
println("  - CrÃ©ation de rapports automatisÃ©s")
println("  - Optimisation de workflows de donnÃ©es")

println("\nðŸš€ PRÃŠT POUR L'Ã‰TAPE SUIVANTE !")
println("ðŸ“† PROCHAINE Ã‰TAPE : 03_visualization.jl - CrÃ©er des visualisations percutantes")
println("   (DataFrames + Plots = Le combo parfait pour l'analyse de donnÃ©es)")
println("   (Conseil : Explorez vos propres datasets avec ces techniques !)")